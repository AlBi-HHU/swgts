{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296045bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.SeqIO.QualityIO import FastqGeneralIterator\n",
    "from scipy.stats import binom\n",
    "from functools import reduce\n",
    "from time import sleep\n",
    "from Bio import SeqIO\n",
    "from subprocess import Popen\n",
    "\n",
    "import mappy as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "import gzip\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "import paramiko\n",
    "import dateutil.parser\n",
    "import re\n",
    "import signal\n",
    "\n",
    "server_machine='www.example.com'\n",
    "\n",
    "TARGET_CONTIGS = {\n",
    "    'mrsa' : 'CP000253.1',\n",
    "    'cov' :'MN908947.3'\n",
    "}\n",
    "\n",
    "MAPPING_PRESETS = {\n",
    "    'wgs' : 'sr',\n",
    "    'ont': 'map-ont'\n",
    "}\n",
    "\n",
    "DATABASES = {\n",
    "    'NEGATIVE' : {\n",
    "        'mrsa' : 'human',\n",
    "        'cov' : 'human'\n",
    "    },\n",
    "    'POSITIVE' : {\n",
    "        'mrsa' : 'mrsa',\n",
    "        'cov' : 'covid'        \n",
    "    },\n",
    "    'COMBINED' : {\n",
    "        'mrsa' : 'mrsa_human',\n",
    "        'cov' : 'cov_human'            \n",
    "    }\n",
    "}\n",
    "\n",
    "CHUNK_SIZES = [1_000,2_500,5_000,10_000,25_000,50_000,100_000,300_000,1_000_000,3_000_000]\n",
    "BUFFER_SIZES = [1_000,2_500,5_000,10_000,25_000,50_000,100_000,300_000,1_000_000,3_000_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf52bf1",
   "metadata": {},
   "source": [
    "# Step 1: Generating of Test Data\n",
    "\n",
    "We have three isolates for Sars-Cov2 and three isolates for MRSA as well as three individuals from the 1000 genomes project. For each isolate we have Nanopore and Illumina reads.\n",
    "We mix 10.000 reads of human reads with 990.000 pathogen reads to create test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca8183",
   "metadata": {},
   "outputs": [],
   "source": [
    "READS_HUMAN = 10_000\n",
    "READS_PATHOGEN = 990_000\n",
    "\n",
    "SAMPLES = {}\n",
    "SAMPLES['HUMAN1']={\n",
    "    'ONT' : '20210519_210512_21-lee-006_PCT0053_2-A9-D9_guppy-5.0.11-sup-prom_fastq_pass.fastq.gz',\n",
    "    'WGS_1' : 'SRR015521_1.filt.fastq.gz' ,\n",
    "    'WGS_2' : 'SRR015521_2.filt.fastq.gz' \n",
    "}\n",
    "SAMPLES['HUMAN2']={\n",
    "    'ONT' : '20211102_211027_21-lee-006_PCT0053_2-A5-D5_guppy-5.0.11-sup-prom_fastq_pass.fastq.gz',\n",
    "    'WGS_1' : 'ERR055344_1.filt.fastq.gz' ,\n",
    "    'WGS_2' : 'ERR055344_2.filt.fastq.gz' \n",
    "}\n",
    "SAMPLES['HUMAN3']={\n",
    "    'ONT' : '20210913_210831_21-lee-006_PCT0053_2-A1-D1_guppy-5.0.11-sup-prom_fastq_pass.fastq.gz',\n",
    "    'WGS_1' : 'ERR016221_1.filt.fastq.gz' ,\n",
    "    'WGS_2' : 'ERR016221_2.filt.fastq.gz' \n",
    "}\n",
    "\n",
    "SAMPLES['MRSA1']={\n",
    "    'ONT' : 'SRR25890280.fastq',\n",
    "    'WGS_1' : 'SRR25890263_1.fastq' ,\n",
    "    'WGS_2' : 'SRR25890263_2.fastq' \n",
    "}\n",
    "SAMPLES['MRSA2']={\n",
    "    'ONT' : 'SRR25890279.fastq',\n",
    "    'WGS_1' : 'SRR25890262_1.fastq' ,\n",
    "    'WGS_2' : 'SRR25890262_2.fastq' \n",
    "}\n",
    "SAMPLES['MRSA3']={\n",
    "    'ONT' : 'SRR25890268.fastq',\n",
    "    'WGS_1' : 'SRR25890261_1.fastq' ,\n",
    "    'WGS_2' : 'SRR25890261_2.fastq' \n",
    "}\n",
    "\n",
    "SAMPLES['COV1']={\n",
    "    'ONT' : 'SRR16555792.fastq',\n",
    "    'WGS_1' : 'SRR16555926_1.fastq' ,\n",
    "    'WGS_2' : 'SRR16555926_2.fastq' \n",
    "}\n",
    "SAMPLES['COV2']={\n",
    "    'ONT' : 'SRR16555741.fastq',\n",
    "    'WGS_1' : 'SRR16555757_1.fastq' ,\n",
    "    'WGS_2' : 'SRR16555757_2.fastq' \n",
    "}\n",
    "SAMPLES['COV3']={\n",
    "    'ONT' : 'SRR16555592.fastq',\n",
    "    'WGS_1' : 'SRR16555506_1.fastq' ,\n",
    "    'WGS_2' : 'SRR16555506_2.fastq' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-sample human for runtime efficiency\n",
    "\n",
    "cmd = f\"seqtk sample -s100 {SAMPLES['HUMAN1']['WGS_1']} 100000 > human1.wgs1.fq\"\n",
    "!{cmd}\n",
    "cmd = f\"seqtk sample -s100 {SAMPLES['HUMAN1']['WGS_2']} 100000 > human1.wgs2.fq\"\n",
    "!{cmd}\n",
    "\n",
    "cmd = f\"seqtk sample -s100 {SAMPLES['HUMAN2']['WGS_1']} 100000 > human2.wgs1.fq\"\n",
    "!{cmd}\n",
    "cmd = f\"seqtk sample -s100 {SAMPLES['HUMAN2']['WGS_2']} 100000 > human2.wgs2.fq\"\n",
    "!{cmd}\n",
    "\n",
    "cmd = f\"seqtk sample -s100 {SAMPLES['HUMAN3']['WGS_1']} 100000 > human3.wgs1.fq\"\n",
    "!{cmd}\n",
    "cmd = f\"seqtk sample -s100 {SAMPLES['HUMAN3']['WGS_2']} 100000 > human3.wgs2.fq\"\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78abaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"seqtk sample -s100 {SAMPLES['HUMAN1']['ONT']} 100000 > human1.ont.fq\"\n",
    "!{cmd}\n",
    "\n",
    "cmd = f\"seqtk sample -s100 {SAMPLES['HUMAN2']['ONT']} 100000 > human2.ont.fq\"\n",
    "!{cmd}\n",
    "\n",
    "cmd = f\"seqtk sample -s100 {SAMPLES['HUMAN3']['ONT']} 100000 > human3.ont.fq\"\n",
    "!{cmd}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_random_reads(files,x,seed=50123):\n",
    "    \n",
    "    #Determine read count total\n",
    "    print('Pass 1, determining total read count')\n",
    "    read_count = 0\n",
    "    fs = []\n",
    "    for pair_idx, f in enumerate(files):\n",
    "        print(f'Adding {f} to pool')\n",
    "        if f.endswith('.gz'):\n",
    "            fs.append(SeqIO.parse(gzip.open(f, \"rt\"),'fastq'))\n",
    "        else:\n",
    "            fs.append(SeqIO.parse(f,'fastq'))\n",
    "    for stream in zip(*fs):\n",
    "        read_count += 1\n",
    "    print(f'Found a total of {read_count} reads')\n",
    "    \n",
    "    random.seed(seed)\n",
    "    selected_ids = set(random.sample(range(read_count),x))\n",
    "    \n",
    "    print('Pass 2, selecting reads')\n",
    "    selected_reads = []\n",
    "    fs = []    \n",
    "    for pair_idx, f in enumerate(files):\n",
    "        print(f'Adding {f} to pool')\n",
    "        if f.endswith('.gz'):\n",
    "            fs.append(SeqIO.parse(gzip.open(f, \"rt\"),'fastq'))\n",
    "        else:\n",
    "            fs.append(SeqIO.parse(f,'fastq'))\n",
    "        \n",
    "    for idx, reads in enumerate(zip(*fs)):\n",
    "        if idx in selected_ids:\n",
    "            selected_ids.remove(idx)\n",
    "            selected_reads.append(reads)\n",
    "            if len(selected_ids) == 0:\n",
    "                break\n",
    "    return selected_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb05b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_suffix(reads,suffix):\n",
    "    for read_idx,paired_reads in enumerate(reads):\n",
    "        for read in paired_reads:\n",
    "            read.id += '_'+suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e968e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_mrsa_1=get_x_random_reads([SAMPLES['MRSA1']['WGS_1'],SAMPLES['MRSA1']['WGS_2']],330_000+62892)\n",
    "#sample 2 only has 204215 reads\n",
    "reads_mrsa_2=get_x_random_reads([SAMPLES['MRSA2']['WGS_1'],SAMPLES['MRSA2']['WGS_2']],204_215)\n",
    "reads_mrsa_3=get_x_random_reads([SAMPLES['MRSA3']['WGS_1'],SAMPLES['MRSA3']['WGS_2']],330_000+62893)\n",
    "\n",
    "reads_human_1=get_x_random_reads(['human1.wgs1.fq','human1.wgs2.fq'],3333)\n",
    "reads_human_2=get_x_random_reads(['human2.wgs1.fq','human2.wgs2.fq'],3333)\n",
    "reads_human_3=get_x_random_reads(['human3.wgs1.fq','human3.wgs2.fq'],3334)\n",
    "\n",
    "reads_mrsa = reads_mrsa_1+reads_mrsa_2+reads_mrsa_3\n",
    "add_suffix(reads_mrsa,'pat')\n",
    "reads_human = reads_human_1+reads_human_2+reads_human_3\n",
    "add_suffix(reads_human,'hom')\n",
    "\n",
    "reads = reads_mrsa+reads_human\n",
    "\n",
    "for pair_idx in range(len(reads[0])):\n",
    "    SeqIO.write((read[pair_idx] for read in reads),f'mrsa_{pair_idx+1}_wgs.fq','fastq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd78db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_cov_1=get_x_random_reads([SAMPLES['COV1']['WGS_1'],SAMPLES['COV1']['WGS_2']],330_000)\n",
    "reads_cov_2=get_x_random_reads([SAMPLES['COV2']['WGS_1'],SAMPLES['COV2']['WGS_2']],330_000)\n",
    "reads_cov_3=get_x_random_reads([SAMPLES['COV3']['WGS_1'],SAMPLES['COV3']['WGS_2']],330_000)\n",
    "\n",
    "reads_human_1=get_x_random_reads(['human1.wgs1.fq','human1.wgs2.fq'],3333,seed=123)\n",
    "reads_human_2=get_x_random_reads(['human2.wgs1.fq','human2.wgs2.fq'],3333,seed=123)\n",
    "reads_human_3=get_x_random_reads(['human3.wgs1.fq','human3.wgs2.fq'],3334,seed=123)\n",
    "\n",
    "reads_cov = reads_cov_1+reads_cov_2+reads_cov_3\n",
    "add_suffix(reads_cov,'pat')\n",
    "reads_human = reads_human_1+reads_human_2+reads_human_3\n",
    "add_suffix(reads_human,'hom')\n",
    "\n",
    "reads = reads_cov+reads_human\n",
    "\n",
    "for pair_idx in range(len(reads[0])):\n",
    "    SeqIO.write((read[pair_idx] for read in reads),f'cov_{pair_idx+1}_wgs.fq','fastq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_mrsa_1=get_x_random_reads([SAMPLES['MRSA1']['ONT']],82_500)\n",
    "reads_mrsa_2=get_x_random_reads([SAMPLES['MRSA2']['ONT']],82_500)\n",
    "reads_mrsa_3=get_x_random_reads([SAMPLES['MRSA3']['ONT']],82_500)\n",
    "\n",
    "reads_human_1=get_x_random_reads(['human1.ont.fq'],833,seed=999)\n",
    "reads_human_2=get_x_random_reads(['human2.ont.fq'],833,seed=999)\n",
    "reads_human_3=get_x_random_reads(['human3.ont.fq'],834,seed=999)\n",
    "\n",
    "reads_mrsa = reads_mrsa_1+reads_mrsa_2+reads_mrsa_3\n",
    "add_suffix(reads_mrsa,'pat')\n",
    "reads_human = reads_human_1+reads_human_2+reads_human_3\n",
    "add_suffix(reads_human,'hom')\n",
    "\n",
    "reads = reads_mrsa+reads_human\n",
    "\n",
    "for pair_idx in range(len(reads[0])):\n",
    "    SeqIO.write((read[pair_idx] for read in reads),f'mrsa_{pair_idx+1}_ont.fq','fastq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05302901",
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_cov_1=get_x_random_reads([SAMPLES['COV1']['ONT']],82_500)\n",
    "reads_cov_2=get_x_random_reads([SAMPLES['COV2']['ONT']],82_500)\n",
    "reads_cov_3=get_x_random_reads([SAMPLES['COV3']['ONT']],82_500)\n",
    "\n",
    "reads_human_1=get_x_random_reads(['human1.ont.fq'],833,seed=333)\n",
    "reads_human_2=get_x_random_reads(['human2.ont.fq'],833,seed=333)\n",
    "reads_human_3=get_x_random_reads(['human3.ont.fq'],834,seed=333)\n",
    "\n",
    "reads_cov = reads_cov_1+reads_cov_2+reads_cov_3\n",
    "add_suffix(reads_cov,'pat')\n",
    "reads_human = reads_human_1+reads_human_2+reads_human_3\n",
    "add_suffix(reads_human,'hom')\n",
    "\n",
    "reads = reads_cov+reads_human\n",
    "\n",
    "for pair_idx in range(len(reads[0])):\n",
    "    SeqIO.write((read[pair_idx] for read in reads),f'cov_{pair_idx+1}_ont.fq','fastq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3db25c",
   "metadata": {},
   "source": [
    "# Step 2: Running ReadItAndKeep and evaulating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f89463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install ReadItAndKeep conda environment according to readme\n",
    "cmd = \"mamba create -y -n readitandkeep -c conda-forge -c bioconda read-it-and-keep\"\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4027f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COV Illumina\n",
    "cmd = 'source ~/miniforge3/etc/profile.d/conda.sh && conda activate readitandkeep && readItAndKeep --ref_fasta MN908947.3.no_poly_A.fa --tech illumina --reads1 cov_1_wgs.fq --reads2 cov_2_wgs.fq -o riak_cov_wgs'\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive: filtered\n",
    "tn = 0\n",
    "fn = 0\n",
    "total_n = 990_000\n",
    "total_p = 10_000\n",
    "\n",
    "for record in SeqIO.parse(gzip.open('riak_cov_wgs.reads_1.fastq.gz','rt'),'fastq'):\n",
    "    if record.id.endswith('_pat'):\n",
    "        tn += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "\n",
    "#filtered falsely     \n",
    "fp = total_n-tn\n",
    "#filtered correctly\n",
    "tp = total_p-fn\n",
    "\n",
    "print(tn,fn,fp,tp)\n",
    "print(sum([tn,fn,fp,tp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COV Ont\n",
    "cmd = 'source ~/miniforge3/etc/profile.d/conda.sh && conda activate readitandkeep && readItAndKeep --ref_fasta MN908947.3.no_poly_A.fa --tech ont --reads1 cov_1_ont.fq -o riak_cov_ont'\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d92b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive: filtered\n",
    "tn = 0\n",
    "fn = 0\n",
    "total_n = 247_500\n",
    "total_p = 2_500\n",
    "\n",
    "for record in SeqIO.parse(gzip.open('riak_cov_ont.reads.fastq.gz','rt'),'fastq'):\n",
    "    if record.id.endswith('_pat'):\n",
    "        tn += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "\n",
    "#filtered falsely     \n",
    "fp = total_n-tn\n",
    "#filtered correctly\n",
    "tp = total_p-fn\n",
    "\n",
    "print(tn,fn,fp,tp)\n",
    "print(sum([tn,fn,fp,tp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MRSA Ont\n",
    "cmd = 'source ~/miniforge3/etc/profile.d/conda.sh && conda activate readitandkeep && readItAndKeep --ref_fasta GCA_000013425.1_ASM1342v1_genomic.fna --tech ont --reads1 mrsa_1_ont.fq -o riak_mrsa_ont'\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f1d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive: filtered\n",
    "tn = 0\n",
    "fn = 0\n",
    "total_n = 247_500\n",
    "total_p = 2_500\n",
    "\n",
    "for record in SeqIO.parse(gzip.open('riak_mrsa_ont.reads.fastq.gz','rt'),'fastq'):\n",
    "    if record.id.endswith('_pat'):\n",
    "        tn += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "\n",
    "#filtered falsely     \n",
    "fp = total_n-tn\n",
    "#filtered correctly\n",
    "tp = total_p-fn\n",
    "\n",
    "print(tn,fn,fp,tp)\n",
    "print(sum([tn,fn,fp,tp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721688ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MRSA WGS\n",
    "cmd = 'source ~/miniforge3/etc/profile.d/conda.sh && conda activate readitandkeep && readItAndKeep --ref_fasta GCA_000013425.1_ASM1342v1_genomic.fna --tech illumina --reads1 mrsa_1_wgs.fq --reads2 mrsa_2_wgs.fq -o riak_mrsa_wgs'\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive: filtered\n",
    "tn = 0\n",
    "fn = 0\n",
    "total_n = 990_000\n",
    "total_p = 10_000\n",
    "\n",
    "for record in SeqIO.parse(gzip.open('riak_mrsa_wgs.reads_1.fastq.gz','rt'),'fastq'):\n",
    "    if record.id.endswith('_pat'):\n",
    "        tn += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "\n",
    "#filtered falsely     \n",
    "fp = total_n-tn\n",
    "#filtered correctly\n",
    "tp = total_p-fn\n",
    "\n",
    "print(tn,fn,fp,tp)\n",
    "print(sum([tn,fn,fp,tp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913216f",
   "metadata": {},
   "source": [
    "# Step 3: Running Hostile and evaulating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e42e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install hostile conda environment according to readme\n",
    "cmd = \"mamba create -y -n hostile -c conda-forge -c bioconda hostile\"\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cov WGS\n",
    "\n",
    "cmd = \"source ~/miniforge3/etc/profile.d/conda.sh && conda activate hostile && hostile clean --fastq1 cov_1_wgs.fq --fastq2 cov_2_wgs.fq --out-dir hostile_output\"\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633c5de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive: filtered\n",
    "tn = 0\n",
    "fn = 0\n",
    "total_n = 990_000\n",
    "total_p = 10_000\n",
    "\n",
    "for record in SeqIO.parse(gzip.open('hostile_output/cov_1_wgs.clean_1.fastq.gz','rt'),'fastq'):\n",
    "    if record.id.endswith('_pat/1'):\n",
    "        tn += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "\n",
    "#filtered falsely     \n",
    "fp = total_n-tn\n",
    "#filtered correctly\n",
    "tp = total_p-fn\n",
    "\n",
    "print(tn,fn,fp,tp)\n",
    "print(sum([tn,fn,fp,tp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f95d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRSA WGS\n",
    "\n",
    "cmd = \"source ~/miniforge3/etc/profile.d/conda.sh && conda activate hostile && hostile clean --fastq1 mrsa_1_wgs.fq --fastq2 mrsa_2_wgs.fq --out-dir hostile_output\"\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6632f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive: filtered\n",
    "tn = 0\n",
    "fn = 0\n",
    "total_n = 990_000\n",
    "total_p = 10_000\n",
    "\n",
    "for record in SeqIO.parse(gzip.open('hostile_output/mrsa_1_wgs.clean_1.fastq.gz','rt'),'fastq'):\n",
    "    if record.id.endswith('_pat/1'):\n",
    "        tn += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "\n",
    "#filtered falsely     \n",
    "fp = total_n-tn\n",
    "#filtered correctly\n",
    "tp = total_p-fn\n",
    "\n",
    "print(tn,fn,fp,tp)\n",
    "print(sum([tn,fn,fp,tp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRSA ONT\n",
    "\n",
    "cmd = \"source ~/miniforge3/etc/profile.d/conda.sh && conda activate hostile && hostile clean --fastq1 mrsa_1_ont.fq --out-dir hostile_output\"\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive: filtered\n",
    "tn = 0\n",
    "fn = 0\n",
    "total_n = 247_500\n",
    "total_p = 2_500\n",
    "\n",
    "for record in SeqIO.parse(gzip.open('hostile_output/mrsa_1_ont.clean.fastq.gz','rt'),'fastq'):\n",
    "    if record.id.endswith('_pat'):\n",
    "        tn += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "\n",
    "#filtered falsely     \n",
    "fp = total_n-tn\n",
    "#filtered correctly\n",
    "tp = total_p-fn\n",
    "\n",
    "print(tn,fn,fp,tp)\n",
    "print(sum([tn,fn,fp,tp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b881fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# COV ONT\n",
    "\n",
    "cmd = \"source ~/miniforge3/etc/profile.d/conda.sh && conda activate hostile && hostile clean --fastq1 cov_1_ont.fq --out-dir hostile_output\"\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f15a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive: filtered\n",
    "tn = 0\n",
    "fn = 0\n",
    "total_n = 247_500\n",
    "total_p = 2_500\n",
    "\n",
    "for record in SeqIO.parse(gzip.open('hostile_output/cov_1_ont.clean.fastq.gz','rt'),'fastq'):\n",
    "    if record.id.endswith('_pat'):\n",
    "        tn += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "\n",
    "#filtered falsely     \n",
    "fp = total_n-tn\n",
    "#filtered correctly\n",
    "tp = total_p-fn\n",
    "\n",
    "print(tn,fn,fp,tp)\n",
    "print(sum([tn,fn,fp,tp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d24da",
   "metadata": {},
   "source": [
    "# Step 4: Running SWGTS and evaulating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b10909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMBINED: Uses a database where one reference is labeled as the target (MINIMAP2_POSITIVE_CONTIG)\n",
    "#NEGATIVE: Only host database is provided and all hits are discarded\n",
    "\n",
    "swgts_tuples = []\n",
    "\n",
    "#We need to evaluate all three operation modes\n",
    "for method in ['NEGATIVE','POSITIVE','COMBINED']:\n",
    "    for pathogen in ['mrsa','cov']:\n",
    "        for technology in ['ont','wgs']:\n",
    "            for buffer_size in BUFFER_SIZES:\n",
    "                #For Illumina WGS we only need one experiment as this is not dependent on buffer size\n",
    "                print(method,pathogen,technology,buffer_size)\n",
    "                if technology == 'wgs' and buffer_size != 3_000_000:\n",
    "                    continue\n",
    "                if os.path.exists(f'../swgts/swgts-submit/outfolder_{method}/{pathogen}_1_{technology}.filtered.fq'):\n",
    "                    print('Detected previous submission, skipping transmission ...')\n",
    "                else:\n",
    "                    FILTER_MODE = 'NEGATIVE' if method == 'NEGATIVE' else 'COMBINED'\n",
    "                    cmd='sed -i -E \"s|(FILTER_MODE:\\\\sstr\\\\s=\\\\s)(.+)|\\\\1\\''+\\\n",
    "                    FILTER_MODE+'\\'|\" ../swgts/input/config_filter.py'\n",
    "                    !{cmd}\n",
    "                    cmd= 'sed -i -E \"s|(MINIMAP2_POSITIVE_CONTIG:\\\\sstr\\\\s=\\\\s)(.+)|\\\\1\\''+\\\n",
    "                    TARGET_CONTIGS[pathogen]+'\\'|\" ../swgts/input/config_filter.py'\n",
    "                    !{cmd}\n",
    "                    cmd='sed -i -E \"s|(MAPPING_PRESET:\\\\sstr\\\\s=\\\\s)(.+)|\\\\1\\''+MAPPING_PRESETS[technology]+'\\'|\" ../swgts/input/config_filter.py'\n",
    "                    !{cmd}\n",
    "                    cmd='sed -i -E \"s|(\\'databases/)(.+)(\\.mmi\\'\\))|\\\\1'+\\\n",
    "                        DATABASES[method][pathogen]+\\\n",
    "                        '\\\\3|\" ../swgts/input/config_filter.py'\n",
    "                    !{cmd}\n",
    "\n",
    "                    cmd='sed -i -E \"s|(MAXIMUM_PENDING_BYTES:\\\\sint\\\\s=\\\\s)(.+)|\\\\1'+\\\n",
    "                        str(300_000_000)+\\\n",
    "                        '|\" ../swgts/input/config_api.py'\n",
    "                    !{cmd}\n",
    "\n",
    "                    cmd='docker compose --project-directory ../swgts down 2>/dev/null'\n",
    "                    !{cmd}\n",
    "                    print('Docker down')\n",
    "                    cmd='docker compose --project-directory ../swgts up --detach --wait 2>/dev/null'\n",
    "                    !{cmd}\n",
    "                    print('Docker up')\n",
    "                    sleep(15)\n",
    "                    #Run Client \n",
    "                    pathprefix = '../../rebenching_revision/'\n",
    "                    add_args_wgs = '' if technology == 'ont' else pathprefix+pathogen+'_2_'+technology+'.fq --paired'\n",
    "\n",
    "                    cmd = f'. ~/miniforge3/etc/profile.d/conda.sh && conda activate swgts-submit && cd ~/sgftp_sandbox/swgts/swgts-submit/ && python3 -m swgts-submit --outfolder outfolder_{method} --verbose --server https://localhost/api {pathprefix}{pathogen}_1_{technology}.fq {add_args_wgs}'\n",
    "                    completed_process = subprocess.run(cmd,shell=True)\n",
    "                    if completed_process.returncode != 0:\n",
    "                        print(completed_process)\n",
    "                        assert(False)\n",
    "\n",
    "                #For Nanopore we need to vary the buffer size since this affects performance\n",
    "                for buffer_size in reversed(\n",
    "                    [1_000,2_500,5_000,10_000,25_000,50_000,100_000,300_000,1_000_000,3_000_000]\n",
    "                ):\n",
    "                    print(buffer_size)\n",
    "\n",
    "                    #positive: filtered\n",
    "                    tn = 0\n",
    "                    fn = 0\n",
    "                    total_n = 247_500 if technology == 'ont' else 990_000\n",
    "                    total_p = 2_500 if technology == 'ont' else 10_000\n",
    "\n",
    "                    for record in SeqIO.parse(\n",
    "                        f'../swgts/swgts-submit/outfolder_{method}/{pathogen}_1_{technology}.filtered.fq','fastq'\n",
    "                    ):\n",
    "                        if len(record) <= buffer_size:\n",
    "                            if record.id.endswith('_pat'):\n",
    "                                tn += 1\n",
    "                            else:\n",
    "                                fn += 1\n",
    "\n",
    "                    #filtered falsely     \n",
    "                    fp = total_n-tn\n",
    "                    #filtered correctly\n",
    "                    tp = total_p-fn\n",
    "\n",
    "                    swgts_tuples.append((technology,pathogen,buffer_size,method,tn,fn,fp,tp,sum([tn,fn,fp,tp])))         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_swgts = pd.DataFrame(swgts_tuples,\n",
    "             columns=[\n",
    "                 'Technology','Pathogen','Buffer Size','Method',\n",
    "                 'TN','FN','FP','TP','Sum'\n",
    "                     ])\n",
    "\n",
    "results_swgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba453c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_swgts['Dataset'] = results_swgts['Technology'].apply(lambda x: x.upper())+'_'+results_swgts['Pathogen'].apply(lambda x: x.upper())\n",
    "results_swgts['Precision'] = results_swgts['TP']/(results_swgts['TP']+results_swgts['FP'])\n",
    "results_swgts['Recall'] = results_swgts['TP']/(results_swgts['TP']+results_swgts['FN'])\n",
    "results_swgts['F1'] = 2*results_swgts['TP']/(2*results_swgts['TP']+results_swgts['FN']+results_swgts['FP'])\n",
    "\n",
    "results_swgts[['Buffer Size','Dataset','Method','TN','FN','FP','TP','Precision','Recall','F1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e039eee7",
   "metadata": {},
   "source": [
    "# Step 5: Buffer Size Simulations\n",
    "\n",
    "We use the InfiniumOmni2 Chip as a basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e01998",
   "metadata": {},
   "source": [
    "## Align real human reads and evaluate SNP coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1105bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Omnium Chip Annotations (SNPs) and transform into 0-based coordinates\n",
    "annotations = pd.read_csv('InfiniumOmni2-5-8v1-5_A1.hg19.annotated.txt',sep='\\t',dtype={\n",
    "    'Chr' : str\n",
    "})\n",
    "annotation_positions = {}\n",
    "\n",
    "for chromosome in annotations['Chr'].unique():\n",
    "    #trasform to 0 based\n",
    "    annotation_positions[chromosome] = set(int(x)-1 for x in annotations[annotations['Chr']==chromosome]['MapInfo'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_length = 0\n",
    "with gzip.open('Homo_sapiens.GRCh37.dna.primary_assembly.fa.gz','rt') as reference_file:\n",
    "    for record in SeqIO.parse(reference_file,'fasta'):\n",
    "        genome_length += len(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398c51b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_count = len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Assuming a genome length of {genome_length} with {snp_count} SNPs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62247dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binomial Expected Values\n",
    "\n",
    "print('Calculating the values based on binomial distribution ...')\n",
    "\n",
    "tuples_binom = []\n",
    "\n",
    "for buffer_size in [500,1000,2500,5000,10000,25000,50000,100000,300000,1000000,3000000]:\n",
    "    mean, var = binom.stats(buffer_size, snp_count/genome_length, moments='mv')\n",
    "    std_dev = math.sqrt(var)\n",
    "    print(buffer_size,mean,std_dev)\n",
    "    tuples_binom.append((buffer_size,mean,std_dev))\n",
    "    \n",
    "binom_df = pd.DataFrame(tuples_binom,columns=['Buffer Size','Mean','Std. Deviation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1856663)\n",
    "result_tuples = []\n",
    "\n",
    "aligner = mp.Aligner('Homo_sapiens.GRCh37.dna.primary_assembly.mmi', preset='map-ont', best_n=1)\n",
    "reads = []\n",
    "with open('human1.ont.fq','r') as file1, open('human2.ont.fq','r') as file2,open('human3.ont.fq','r') as file3:\n",
    "    print('1')\n",
    "    reads += list(\n",
    "        x[1] for x in FastqGeneralIterator(file1)\n",
    "    )\n",
    "    print('2')\n",
    "    reads+=list(\n",
    "        x[1] for x in FastqGeneralIterator(file2)\n",
    "    )\n",
    "    print('3')\n",
    "    reads+=list(\n",
    "        x[1] for x in FastqGeneralIterator(file3)\n",
    "    )\n",
    "print('All Nanopore Reads in Memory')\n",
    "\n",
    "for target_buffer_size in [500,1000,2500,5000,10000,25000,50000,100000,300000,1000000,3000000]:\n",
    "    print(target_buffer_size)\n",
    "\n",
    "    for repeat in range(10):\n",
    "        hits = set()\n",
    "\n",
    "        random.shuffle(reads)\n",
    "        buffer_size = 0\n",
    "        while buffer_size < target_buffer_size:\n",
    "            for read in reads:\n",
    "                excess = (len(read) + buffer_size)-target_buffer_size\n",
    "                if excess > 0:\n",
    "                    read = read[:-excess]\n",
    "                for hit in aligner.map(read):\n",
    "                    if hit.ctg in annotation_positions:\n",
    "                        for pos in range(hit.r_st,hit.r_en+1):\n",
    "                            if pos in annotation_positions[hit.ctg]:\n",
    "                                hits.add((hit.ctg,pos))\n",
    "                buffer_size += len(read)\n",
    "\n",
    "        result_tuples.append(\n",
    "            ('Nanopore',repeat,target_buffer_size,buffer_size,len(hits))\n",
    "        )\n",
    "\n",
    "\n",
    "aligner = mp.Aligner('Homo_sapiens.GRCh37.dna.primary_assembly.mmi', preset='sr', best_n=1)\n",
    "reads = []\n",
    "with open('human1.wgs1.fq','r') as file11,\\\n",
    "    open('human2.wgs1.fq','r') as file21,\\\n",
    "    open('human3.wgs1.fq','r') as file31,\\\n",
    "    open('human1.wgs2.fq','r') as file12,\\\n",
    "    open('human2.wgs2.fq','r') as file22,\\\n",
    "    open('human3.wgs2.fq','r') as file32:\n",
    "    print('1')\n",
    "    reads += list(\n",
    "        zip(\n",
    "            (x[1] for x in FastqGeneralIterator(file11)),\n",
    "            (x[1] for x in FastqGeneralIterator(file12))                \n",
    "        )\n",
    "    )\n",
    "    print('2')\n",
    "    reads += list(\n",
    "        zip(\n",
    "            (x[1] for x in FastqGeneralIterator(file21)),\n",
    "            (x[1] for x in FastqGeneralIterator(file22))                \n",
    "        )\n",
    "    )\n",
    "    print('3')\n",
    "    reads += list(\n",
    "        zip(\n",
    "            (x[1] for x in FastqGeneralIterator(file31)),\n",
    "            (x[1] for x in FastqGeneralIterator(file32))                \n",
    "        )\n",
    "    )\n",
    "print('All Illumina Reads in Memory')\n",
    "\n",
    "for target_buffer_size in [500,1000,2500,5000,10000,25000,50000,100000,300000,1000000,3000000]:\n",
    "    print(target_buffer_size)\n",
    "    for repeat in range(10):\n",
    "        hits = set()\n",
    "\n",
    "        random.shuffle(reads)\n",
    "        buffer_size = 0\n",
    "        while buffer_size < target_buffer_size:\n",
    "            for read1,read2 in reads:\n",
    "                excess = (len(read1)+len(read2) + buffer_size)-target_buffer_size\n",
    "                if excess > 0:\n",
    "                    if excess > len(read2):\n",
    "                        excess_read1 = excess-len(read2)\n",
    "                        read2 = ''\n",
    "                        read1 = read1[:-excess_read1]\n",
    "                    else:\n",
    "                        read2 = read2[:-excess]\n",
    "                for hit in aligner.map(read1,read2):\n",
    "                    if hit.ctg in annotation_positions:\n",
    "                        for pos in range(hit.r_st,hit.r_en+1):\n",
    "                            if pos in annotation_positions[hit.ctg]:\n",
    "                                hits.add((hit.ctg,pos))\n",
    "                buffer_size += (len(read1)+len(read2))\n",
    "\n",
    "        result_tuples.append(\n",
    "            ('Illumina',repeat,target_buffer_size,buffer_size,len(hits))\n",
    "        )\n",
    "    \n",
    "aligner = None\n",
    "reads = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c3a54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_df = pd.DataFrame(result_tuples,columns=['Method','Repeat','Target Buffer Size','Buffer Size','Hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04dbed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediates\n",
    "simulation_df.to_csv('simulation_human_snp.csv',index=False)\n",
    "binom_df.to_csv('binom_human_snp.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3069ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_df = pd.read_csv('simulation_human_snp.csv')\n",
    "binom_df = pd.read_csv('binom_human_snp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde2283f",
   "metadata": {},
   "source": [
    "## Create Paper Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "binom_df['Low'] = binom_df['Mean']-binom_df['Std. Deviation']\n",
    "binom_df['High'] = binom_df['Mean']+binom_df['Std. Deviation']\n",
    "\n",
    "\n",
    "(alt.Chart(simulation_df).mark_boxplot().encode(\n",
    "    x=alt.X('Buffer Size:O',title='Buffer Size (Bases)'),\n",
    "    y=alt.Y(\n",
    "        'Hits:Q',\n",
    "        title='Covered Panel SNPs',\n",
    "        scale=alt.Scale(type='symlog'\n",
    "                       ),\n",
    "        axis=alt.Axis(values=[0,10,50,100,500,1000,1500,2000])\n",
    "    ),\n",
    "    xOffset='Method:N',\n",
    "    color='Method:N'\n",
    ")+alt.Chart(binom_df).mark_line(color='black').encode(\n",
    "    x='Buffer Size:O',\n",
    "    y='Mean'\n",
    ")+alt.Chart(binom_df).mark_point(\n",
    "    color='black',\n",
    "    filled=True,\n",
    "    fill='white',\n",
    "    strokeWidth=2,\n",
    "    opacity=1,\n",
    "    stroke='black'\n",
    ").encode(\n",
    "    x='Buffer Size:O',\n",
    "    y='Mean'\n",
    ")+alt.Chart(pd.DataFrame([(92)],columns=['Y'])\n",
    "           ).mark_rule(strokeDash=[2,1]).encode(\n",
    "    y='Y'\n",
    ")\n",
    "\n",
    ").properties(width=300).configure_legend(\n",
    "    symbolFillColor=None\n",
    ").resolve_scale(color='shared')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68ea15",
   "metadata": {},
   "source": [
    "## Combine tables into readable supplementary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33479df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df_transformed = simulation_df.groupby(\n",
    "    ['Method','Buffer Size']\n",
    ")['Hits'].agg(\n",
    "    Mean=np.mean,Std=np.std\n",
    ").reset_index().pivot(\n",
    "    index='Buffer Size',\n",
    "    columns='Method',\n",
    "    values=['Mean','Std']\n",
    ")\n",
    "sim_df_transformed.columns = [col[0] + f\"_{col[1]}\" for col in sim_df_transformed.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f39535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Tables and add Mean/Std. Deviation\n",
    "final = pd.merge(sim_df_transformed,binom_df,on='Buffer Size').sort_values(by='Buffer Size')\n",
    "#Generate Buffer Size Modelig Table\n",
    "final.to_csv('BufferSizeModeling.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ae90f5",
   "metadata": {},
   "source": [
    "# Step 6: Performance Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d97d61c",
   "metadata": {},
   "source": [
    "## Subsample the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce7642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(270923)\n",
    "\n",
    "for pathogen in ['mrsa','cov']:\n",
    "    for technology in ['ont','wgs']:\n",
    "        print(technology,pathogen)\n",
    "        #Total read count / bases should be able to fill the buffer\n",
    "        SAMPLE_SIZE = 50_000 if technology == 'wgs' else 20_000\n",
    "        if os.path.exists(f'benchmark_{pathogen}_{technology}_1.fq'):\n",
    "            print(f'Skipping {pathogen}/{technology}, already existing ...')\n",
    "        if technology == 'wgs':\n",
    "            records = []\n",
    "            for r1,r2 in zip(SeqIO.parse(f'{pathogen}_1_wgs.fq','fastq'),SeqIO.parse(f'{pathogen}_2_wgs.fq','fastq')):\n",
    "                records.append((r1,r2))\n",
    "            random_sample = random.sample(records,SAMPLE_SIZE)\n",
    "            r1s = (x[0] for x in random_sample)\n",
    "            r2s = (x[1] for x in random_sample)\n",
    "            SeqIO.write(r1s,f'benchmark_{pathogen}_{technology}_1.fq','fastq')\n",
    "            SeqIO.write(r2s,f'benchmark_{pathogen}_{technology}_2.fq','fastq')\n",
    "        elif technology == 'ont':\n",
    "            records = []\n",
    "            for r in SeqIO.parse(f'{pathogen}_1_ont.fq','fastq'):\n",
    "                records.append(r)\n",
    "            random_sample = random.sample(records,SAMPLE_SIZE)\n",
    "            SeqIO.write(random_sample,f'benchmark_{pathogen}_{technology}_1.fq','fastq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7519d6",
   "metadata": {},
   "source": [
    "## Measuring Chunk Size and Buffer Size Impact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bde4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPEATS = 5\n",
    "\n",
    "method = 'NEGATIVE'\n",
    "\n",
    "for chunk_size in reversed(CHUNK_SIZES):\n",
    "    for buffer_size in reversed(BUFFER_SIZES):\n",
    "        if chunk_size > buffer_size:\n",
    "            continue\n",
    "        for technology in ['ont','wgs']:\n",
    "            if technology == 'wgs':\n",
    "                continue\n",
    "            for pathogen in ['mrsa','cov']:\n",
    "                if pathogen == 'mrsa':\n",
    "                    continue\n",
    "\n",
    "                print(chunk_size,buffer_size,pathogen,technology)\n",
    "                if os.path.exists(f'bufferchunk/{chunk_size}_{buffer_size}_{pathogen}_{technology}.csv'):\n",
    "                    continue\n",
    "\n",
    "                client = paramiko.SSHClient()\n",
    "                client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "                try:\n",
    "\n",
    "                    client.connect(hostname=server_machine, username='root')\n",
    "                    #Set Config Accordingly\n",
    "                    FILTER_MODE = 'NEGATIVE' if method == 'NEGATIVE' else 'COMBINED'\n",
    "                    client.exec_command(\n",
    "                        'sed -i -E \"s|(FILTER_MODE:\\\\sstr\\\\s=\\\\s)(.+)|\\\\1\\''+FILTER_MODE+'\\'|\" /srv/swgts-deploy/input/config_filter.py'\n",
    "                    )\n",
    "                    client.exec_command(\n",
    "                        'sed -i -E \"s|(MINIMAP2_POSITIVE_CONTIG:\\\\sstr\\\\s=\\\\s)(.+)|\\\\1\\''+TARGET_CONTIGS[pathogen]+'\\'|\" /srv/swgts-deploy/input/config_filter.py'\n",
    "                    )\n",
    "                    client.exec_command(\n",
    "                        'sed -i -E \"s|(MAPPING_PRESET:\\\\sstr\\\\s=\\\\s)(.+)|\\\\1\\''+MAPPING_PRESETS[technology]+'\\'|\" /srv/swgts-deploy/input/config_filter.py'\n",
    "                    )\n",
    "                    client.exec_command(\n",
    "                        'sed -i -E \"s|(\\'databases/)(.+)(\\.mmi\\'\\))|\\\\1'+\\\n",
    "                        DATABASES[method][pathogen]+\\\n",
    "                        '\\\\3|\" /srv/swgts-deploy/input/config_filter.py'\n",
    "                    )\n",
    "\n",
    "                    #Adjust buffer\n",
    "                    client.exec_command(\n",
    "                        'sed -i -E \"s|(MAXIMUM_PENDING_BYTES:\\\\sint\\\\s=\\\\s)(.+)|\\\\1'+str(buffer_size)+'|\" /srv/swgts-deploy/input/config_api.py'\n",
    "                    ) \n",
    "\n",
    "                    stdin, stdout, stderr = client.exec_command(\n",
    "                        'docker compose --project-directory /srv/swgts-deploy down -v'\n",
    "                    )\n",
    "                    stdout.readlines() #Pseudo-Block\n",
    "                    stdin, stdout, stderr = client.exec_command(\n",
    "                        'docker compose --project-directory /srv/swgts-deploy up --detach --wait'\n",
    "                    )                \n",
    "                    stdout.readlines() #Pseudo-Block\n",
    "\n",
    "                except:\n",
    "                    print(\"[!] Cannot connect to the SSH Server\")\n",
    "                    assert(False)\n",
    "\n",
    "                client.close()\n",
    "                #Run Client \n",
    "                sleep(12) #Server needs a couple of seconds post docker return code\n",
    "                pathprefix = '../../rebenching_revision/'\n",
    "                add_args_wgs = '' if technology == 'ont' else pathprefix+'benchmark_'+pathogen+'_'+technology+'_2'+'.fq --paired'\n",
    "\n",
    "                cmd = f'. ~/miniforge3/etc/profile.d/conda.sh && conda activate swgts-submit && cd ~/sgftp_sandbox/swgts/swgts-submit/ && python3 -m swgts-submit --count {chunk_size} --server https://{server_machine}/api {pathprefix}benchmark_{pathogen}_{technology}_1.fq {add_args_wgs} --verbose'\n",
    "                \n",
    "                with open(f'bufferchunk/{chunk_size}_{buffer_size}_{pathogen}_{technology}.csv','w') as outfile,\\\n",
    "                     open(f'bufferchunk/{chunk_size}_{buffer_size}_{pathogen}_{technology}.output','w') as outstream:\n",
    "\n",
    "                    #Repeat X times\n",
    "                    for repeat in range(REPEATS+1):\n",
    "                        print(f'Repeat: {repeat}')\n",
    "                        pre = time.time()\n",
    "                        completed_process = subprocess.run(cmd,shell=True,stdout = outstream,stderr=outstream)\n",
    "                        if completed_process.returncode != 0:\n",
    "                            print(completed_process)\n",
    "                            assert(False)\n",
    "                        post = time.time()\n",
    "                        if repeat == 0:\n",
    "                            #Discard first measurement to get rid of startup effects\n",
    "                            continue\n",
    "                        elapsed = post-pre\n",
    "                        outfile.write(\n",
    "                            f'{pathogen},{elapsed},{repeat},{chunk_size},{buffer_size},{technology}\\n'\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_tuples = []\n",
    "server_tuples = []\n",
    "lengths_histo = []\n",
    "\n",
    "#Parse exact base counts for rates\n",
    "for pathogen in ['mrsa','cov']:\n",
    "    #ont\n",
    "    total = 0\n",
    "    server = {x:0 for x in BUFFER_SIZES}\n",
    "    for read in SeqIO.parse(f'benchmark_{pathogen}_ont_1.fq','fastq'):\n",
    "        lengths_histo.append((pathogen,len(read),read.id.endswith('hom')))\n",
    "        total += len(read)\n",
    "        for buffer_size in BUFFER_SIZES:\n",
    "            if len(read) <= buffer_size:\n",
    "                server[buffer_size] += len(read)\n",
    "    length_tuples.append((pathogen,'ont',total))\n",
    "    for k,v in server.items():\n",
    "        server_tuples.append(\n",
    "            (pathogen,'ont',k,v/total)\n",
    "        )\n",
    "\n",
    "    #wgs\n",
    "    total = 0\n",
    "    server = {x:0 for x in BUFFER_SIZES}\n",
    "    for r1,r2 in zip(\n",
    "        SeqIO.parse(f'benchmark_{pathogen}_wgs_1.fq','fastq'),\n",
    "        SeqIO.parse(f'benchmark_{pathogen}_wgs_2.fq','fastq')\n",
    "    ):\n",
    "        total += len(r1)+len(r2)\n",
    "        for buffer_size in BUFFER_SIZES:\n",
    "            if (len(r1)+len(r2)) <= buffer_size:\n",
    "                server[buffer_size] += (len(r1)+len(r2))\n",
    "    length_tuples.append((pathogen,'wgs',total))\n",
    "    for k,v in server.items():\n",
    "        server_tuples.append(\n",
    "            (pathogen,'wgs',k,v/total)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e7580",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df = pd.DataFrame(length_tuples,columns=['Pathogen','Technology','Bases'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b60336",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_df = pd.DataFrame(server_tuples,columns=['Pathogen','Technology','Buffer Size','Fraction Processed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "frames = []\n",
    "for chunk_size in CHUNK_SIZES:\n",
    "    for buffer_size in BUFFER_SIZES:\n",
    "        for technology in ['ont','wgs']:\n",
    "            if chunk_size > buffer_size:\n",
    "                continue\n",
    "\n",
    "            for pathogen in ['mrsa','cov']:\n",
    "                if os.path.exists(f'bufferchunk/{chunk_size}_{buffer_size}_{pathogen}_{technology}.csv'):\n",
    "                    frames.append(\n",
    "                        pd.read_csv(\n",
    "                            f'bufferchunk/{chunk_size}_{buffer_size}_{pathogen}_{technology}.csv',\n",
    "                            header=None,sep=',',\n",
    "                            names=['Pathogen','Time (s)','Repeat','Chunk Size','Buffer Size','Technology']\n",
    "                        )\n",
    "                    )\n",
    "                \n",
    "\n",
    "benchmark = pd.concat(frames)\n",
    "benchmark = pd.merge(benchmark,length_df,on=['Pathogen','Technology'],how='left')\n",
    "benchmark = pd.merge(benchmark,server_df,on=['Pathogen','Technology','Buffer Size'],how='left')\n",
    "\n",
    "benchmark['Bases/s'] = benchmark['Bases'] / benchmark['Time (s)'] \n",
    "benchmark['KBases/s'] = round(benchmark['Bases/s'] / 1000)\n",
    "benchmark['Percentage Processed'] = benchmark['Fraction Processed'].apply(lambda x : round(x*100,2))\n",
    "\n",
    "benchmark['Technology'] = benchmark['Technology'].map(\n",
    "    {\n",
    "        'ont' : 'ONT',\n",
    "        'wgs' : 'Illumina'\n",
    "    }\n",
    ")\n",
    "\n",
    "benchmark['Pathogen'] = benchmark['Pathogen'].map(\n",
    "    {\n",
    "        'mrsa' : 'MRSA',\n",
    "        'cov' : 'CoV'\n",
    "    }\n",
    ")\n",
    "\n",
    "aggregated_benchmark = benchmark.groupby(\n",
    "    ['Pathogen','Technology','Buffer Size','Chunk Size','Percentage Processed'],as_index=False\n",
    ")['Time (s)','Bases/s','KBases/s'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0fd6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_benchmark.to_csv(\n",
    "    f'BenchmarkChunkBuffer_{date.today()}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4919da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_distributions = pd.DataFrame(lengths_histo,columns=['Pathogen','Length','Human'])\n",
    "def get_bin(x):\n",
    "    for size in BUFFER_SIZES:\n",
    "        if x <= size:\n",
    "            return size\n",
    "length_distributions['Bin'] = length_distributions['Length'].apply( get_bin)\n",
    "length_distributions['Read Type'] = length_distributions['Human'].map({\n",
    "    False : 'Pathogen',\n",
    "    True : 'Human'\n",
    "})\n",
    "\n",
    "length_distributions['Pathogen'] = length_distributions['Pathogen'].map(\n",
    "    {\n",
    "        'mrsa' : 'MRSA',\n",
    "        'cov' : 'CoV'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae45c3",
   "metadata": {},
   "source": [
    "## Creating the Panel for the Paper Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e60af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = benchmark.groupby(['Buffer Size','Chunk Size','Pathogen','Technology'],as_index=False)['KBases/s'].median()\n",
    "alt.data_transformers.disable_max_rows()\n",
    "c1 =(alt.Chart(\n",
    "    data, height=300,width=300\n",
    ").mark_rect().encode(\n",
    "    x=alt.X('Buffer Size:O',title=None),\n",
    "    y='Chunk Size:O',\n",
    "    color=alt.Color('KBases/s',scale=alt.Scale(type='symlog'),title='Kilobases / s')\n",
    ")+alt.Chart(\n",
    "    data, height=300,width=300\n",
    ").mark_text().encode(\n",
    "    x=alt.X('Buffer Size:O',title=None,axis=None),\n",
    "    y='Chunk Size:O',\n",
    "    text='KBases/s'\n",
    ")).facet(\n",
    "    row='Technology',\n",
    "    column='Pathogen'\n",
    ")\n",
    "c2 = alt.Chart(length_distributions,height=300,width=300).mark_bar().encode(\n",
    "    x=alt.X('Bin:O',title='Buffer Size'),\n",
    "    y=alt.Y('count(Bin)',scale=alt.Scale(type='symlog'),title='Additional Reads <= Buffer',\n",
    "            axis=alt.Axis(values=[0,10,100,1000,10000,20000],format=\".1e\")),\n",
    "    xOffset='Read Type',\n",
    "    color='Read Type',\n",
    "    column=alt.Column('Pathogen',title=None,header=None),\n",
    "    tooltip=['count(Bin)']\n",
    ")\n",
    "\n",
    "c1&c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14978cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = benchmark.groupby(\n",
    "    ['Buffer Size','Chunk Size','Pathogen','Technology'],as_index=False\n",
    ")['KBases/s'].median().groupby(\n",
    "    ['Buffer Size','Pathogen','Technology'],as_index=False\n",
    ").max()\n",
    "data['Dataset'] = data['Technology']+'/'+data['Pathogen']\n",
    "\n",
    "c1 = alt.Chart(benchmark,height=300).mark_rect(point=True).encode(\n",
    "    x='Buffer Size:O',\n",
    "    color='mean(KBases/s)',\n",
    "    y='Chunk Size:O'\n",
    ")+alt.Chart(benchmark,height=300).mark_text().encode(\n",
    "    x='Buffer Size:O',\n",
    "    text=alt.Text('mean(KBases/s)',format=\".0f\"),\n",
    "    y='Chunk Size:O'\n",
    ")\n",
    "\n",
    "\n",
    "c1.properties(width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e011f6",
   "metadata": {},
   "source": [
    "## Evaluating Client/Server Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c481fe",
   "metadata": {},
   "source": [
    "We evaluate the maximum possible bandwith between the machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c22d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = paramiko.SSHClient()\n",
    "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "try:\n",
    "    client.connect(hostname=server_machine, username='root')\n",
    "    client.exec_command('iperf3 -s -p 443 -1')\n",
    "    cmd = f'iperf3 --verbose -c {server_machine} -p 443 > iperf3.output'\n",
    "    print('Client Starting ...')\n",
    "    !{cmd}\n",
    "except:\n",
    "    print(\"[!] Cannot connect to the SSH Server\")\n",
    "    exit()\n",
    "    \n",
    "client.close()\n",
    "\n",
    "with open('iperf3.output','r') as infile:\n",
    "    print(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_COUNTS=range(1,31)#[1,2,3,20,30]#range(10,11)\n",
    "MEASUREMENT_PERIOD = 30\n",
    "\n",
    "CONFIGURATIONS = {\n",
    "1:  {'chunk_size' : 2_500, 'buffer_size' : 10_000, 'pathogen' : 'mrsa','threads' : 7,'technology':'wgs'},\n",
    "2:    {'chunk_size' : 2_500, 'buffer_size' : 10_000, 'pathogen' : 'mrsa','threads' : 3,'technology':'wgs'},\n",
    "4:    {'chunk_size' : 25_000, 'buffer_size' : 100_000, 'pathogen' : 'cov','threads' : 3,'technology':'ont'},\n",
    "#5:    {'chunk_size' : 25_000, 'buffer_size' : 100_000, 'pathogen' : 'cov','threads' : 5,'technology':'ont'},\n",
    "6:    {'chunk_size' : 25_000, 'buffer_size' : 100_000, 'pathogen' : 'cov','threads' : 7,'technology':'ont'},\n",
    "#7:    {'chunk_size' : 25_000, 'buffer_size' : 100_000, 'pathogen' : 'cov','threads' : 9,'technology':'ont'}\n",
    "#8:    {'chunk_size' : 25_000, 'buffer_size' : 100_000, 'pathogen' : 'cov','threads' : 1,'technology':'ont'}\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "description_map = { \n",
    "    1: '2.500 Chunk Size 10.000 Buffer Size MRSA Illumina 7 Threads',\n",
    "    2: '2.500 Chunk Size 10.000 Buffer Size MRSA Illumina 3 Threads',\n",
    "    3: '25.000 Chunk Size 100.000 Buffer Size COV ONT 7 Threads',\n",
    "    4: '25.000 Chunk Size 100.000 Buffer Size COV ONT 3 Threads',\n",
    "    5: '25.000 Chunk Size 100.000 Buffer Size COV ONT 5 Threads',\n",
    "    6: '25.000 Chunk Size 100.000 Buffer Size COV ONT 7 Threads',\n",
    "    7: '25.000 Chunk Size 100.000 Buffer Size COV ONT 9 Threads',\n",
    "    8: '25.000 Chunk Size 100.000 Buffer Size COV ONT 1 Threads',\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af27be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'NEGATIVE'\n",
    "\n",
    "result_tuples = []\n",
    "\n",
    "for idx,configuration in CONFIGURATIONS.items():\n",
    "    \n",
    "    print(f'Running configuration {idx} : {configuration} ')\n",
    "    \n",
    "    pathogen = configuration['pathogen']\n",
    "    technology = configuration['technology']\n",
    "    buffer_size = configuration['buffer_size']\n",
    "    chunk_size = configuration['chunk_size']\n",
    "    \n",
    "    processes = {}\n",
    "\n",
    "    vmstat_client = paramiko.SSHClient()\n",
    "    vmstat_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    \n",
    "    dockerstat_client = paramiko.SSHClient()\n",
    "    dockerstat_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    \n",
    "    ifstat_client = paramiko.SSHClient()\n",
    "    ifstat_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "    redis_client = paramiko.SSHClient()\n",
    "    redis_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    \n",
    "    client = paramiko.SSHClient()\n",
    "    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    \n",
    "    try:\n",
    "\n",
    "        client.connect(hostname=server_machine, username='root')\n",
    "       \n",
    "        #Set Config Accordingly\n",
    "        FILTER_MODE = 'NEGATIVE' if method == 'NEGATIVE' else 'COMBINED'\n",
    "        client.exec_command(\n",
    "            'sed -i -E \"s|(FILTER_MODE:\\\\sstr\\\\s=\\\\s)(.+)|\\\\1\\''+FILTER_MODE+'\\'|\" /srv/swgts-deploy/input/config_filter.py'\n",
    "        )\n",
    "        client.exec_command(\n",
    "            'sed -i -E \"s|(MINIMAP2_POSITIVE_CONTIG:\\\\sstr\\\\s=\\\\s)(.+)|\\\\1\\''+TARGET_CONTIGS[pathogen]+'\\'|\" /srv/swgts-deploy/input/config_filter.py'\n",
    "        )\n",
    "        client.exec_command(\n",
    "            'sed -i -E \"s|(MAPPING_PRESET:\\\\sstr\\\\s=\\\\s)(.+)|\\\\1\\''+MAPPING_PRESETS[technology]+'\\'|\" /srv/swgts-deploy/input/config_filter.py'\n",
    "        )\n",
    "        \n",
    "        client.exec_command(\n",
    "            'sed -i -E \"s|(\\'databases/)(.+)(\\.mmi\\'\\))|\\\\1'+\\\n",
    "            DATABASES[method][pathogen]+\\\n",
    "            '\\\\3|\" /srv/swgts-deploy/input/config_filter.py'\n",
    "        )\n",
    "\n",
    "        #Adjust buffer\n",
    "        client.exec_command(\n",
    "            'sed -i -E \"s|(MAXIMUM_PENDING_BYTES:\\\\sint\\\\s=\\\\s)(.+)|\\\\1'+str(buffer_size)+'|\" /srv/swgts-deploy/input/config_api.py'\n",
    "        ) \n",
    "        \n",
    "        #Adjust worker threadsMC\n",
    "        client.exec_command(\n",
    "            'sed -i -E \"s|(WORKER_THREADS:\\\\sint\\\\s=\\\\s)(.+)|\\\\1'+str(configuration['threads'])+'|\" /srv/swgts-deploy/input/config_filter.py'\n",
    "        ) \n",
    "\n",
    "        \n",
    "        for client_count in CLIENT_COUNTS:\n",
    "            \n",
    "            print(f'Using {client_count} clients')\n",
    "            \n",
    "            stdin, stdout, stderr = client.exec_command(\n",
    "                'docker compose --project-directory /srv/swgts-deploy down -v'\n",
    "            )\n",
    "            stdout.readlines() #Pseudo-Block\n",
    "            stdin, stdout, stderr = client.exec_command(\n",
    "                'docker compose --project-directory /srv/swgts-deploy up --detach'\n",
    "            )                \n",
    "            stdout.readlines() #Pseudo-Block\n",
    "            #Run Client \n",
    "            sleep(12) #Server needs a couple of seconds post docker return code\n",
    "\n",
    "\n",
    "            !{f'curl --insecure https://{server_machine}/api/server-status'}\n",
    "            \n",
    "            logfiles = []\n",
    "            \n",
    "            vmstat_client.connect(hostname=server_machine, username='root')\n",
    "            ifstat_client.connect(hostname=server_machine, username='root')\n",
    "            redis_client.connect(hostname=server_machine, username='root')\n",
    "            dockerstat_client.connect(hostname=server_machine, username='root')\n",
    "\n",
    "            \n",
    "            vmstat_client.exec_command(\n",
    "                f'vmstat -n -t 1 > benchmarks/benchmark_{idx}_{client_count}_vm.txt',\n",
    "                get_pty=True\n",
    "            )\n",
    "\n",
    "            ifstat_client.exec_command(\n",
    "                f'while true; do echo \"$(date)\" && iftop -i enX0 -t -s 1 | grep \"Total send and receive rate\" | tr \"\\\\n\" \"\\\\t\" || break; done > /root/benchmarks/benchmark_{idx}_{client_count}_if.txt\\n',\n",
    "                get_pty=True\n",
    "            )\n",
    "            \n",
    "            dockerstat_client.exec_command(\n",
    "                f'while true; do docker stats --format \"{{{{.Name}}}}\\t{{{{.CPUPerc}}}}\\t{{{{.NetIO}}}}\\t{{{{.BlockIO}}}}\" || break; done > /root/benchmarks/benchmark_{idx}_{client_count}_do.txt\\n',\n",
    "                get_pty=True\n",
    "            )\n",
    "            \n",
    "            \n",
    "            for clientidx in range(client_count):\n",
    "                \n",
    "                logfile = open(f'benchmarks_client_counts/benchmark_{idx}_{client_count}_{clientidx}_cl.txt','w')\n",
    "                \n",
    "                processes[clientidx] =  Popen(\n",
    "                    f'. ~/miniforge3/etc/profile.d/conda.sh && conda activate swgts-submit'+\n",
    "                    f' && echo \"$(date)\" && cd ~/sgftp_sandbox/swgts/swgts-submit/ &&'+\n",
    "                    f' python3 '+\n",
    "                    f'-m swgts-submit --count {chunk_size} --verbose --server https://{server_machine}/api'+\n",
    "                    f' {pathprefix}{pathogen}_1_{technology}.fq {add_args_wgs} 2>&1 | tr \"\\\\015\" \"\\n\"',\n",
    "                    shell=True,\n",
    "                    preexec_fn= os.setsid, \n",
    "                    stdout = logfile,\n",
    "                    stderr = logfile\n",
    "                )               \n",
    "                \n",
    "                \n",
    "                logfiles.append(logfile)\n",
    "                \n",
    "            sleep(MEASUREMENT_PERIOD)\n",
    "            \n",
    "            stdin, stdout, stderr = redis_client.exec_command(\n",
    "                f'docker exec -it swgts-deploy-redis-1 redis-cli GET stats:bases > benchmarks/benchmark_{idx}_{client_count}_bs.txt',\n",
    "                get_pty=True\n",
    "            )\n",
    "            stdout.readlines() #Pseudo-Block\n",
    "                        \n",
    "            vmstat_client.close()\n",
    "            ifstat_client.close()\n",
    "            redis_client.close()\n",
    "            dockerstat_client.close()\n",
    "            \n",
    "            for p in processes.values():\n",
    "                os.killpg(os.getpgid(p.pid), signal.SIGTERM) \n",
    "\n",
    "            for l in logfiles:\n",
    "                l.close()\n",
    "                \n",
    "            #break\n",
    "        client.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        redis_client.close()\n",
    "        vmstat_client.close()\n",
    "        ifstat_client.close()\n",
    "        dockerstat_client.close()\n",
    "        for p in processes.values():\n",
    "            os.killpg(os.getpgid(p.pid), signal.SIGTERM) \n",
    "        for l in logfiles:\n",
    "            l.close()\n",
    "    #break\n",
    "            \n",
    "processes = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f254a8",
   "metadata": {},
   "source": [
    "## Transfering server-side benchmark files for local evaluation\n",
    "\n",
    "Requires rsync to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e4f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f'rsync root@{server_machine}:~/benchmarks/benchmark* benchmarks_client_counts --info=progress2'\n",
    "\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c24c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = []\n",
    "\n",
    "for s in CONFIGURATIONS:\n",
    "    for c in CLIENT_COUNTS:\n",
    "        try:\n",
    "            #Step 1: Get Client\n",
    "            with open(f'benchmarks_client_counts/benchmark_{s}_{c}_if.txt','r') as if_file:\n",
    "                rates = []\n",
    "                dead_clients = 0\n",
    "                timeouts = 0\n",
    "                wrong_estimates = 0\n",
    "                for cidx in range(c):\n",
    "                    with open(f'benchmarks_client_counts/benchmark_{s}_{c}_{cidx}_cl.txt','r') as cl_file:\n",
    "                        #skip first and last 1/5 of lines to avoid measuring startup/end effects\n",
    "                        lines = cl_file.readlines()\n",
    "                        fifth = len(lines)//5\n",
    "                        date = dateutil.parser.parse(lines[0])\n",
    "\n",
    "                        #Scan for crashes\n",
    "                        crash_found = False\n",
    "                        for line in lines:\n",
    "                            if line.startswith('Traceback'):\n",
    "                                dead_clients += 1\n",
    "                                crash_found = True\n",
    "                                break\n",
    "                        if crash_found:\n",
    "                            break\n",
    "\n",
    "                        current_line_is_timeout = False\n",
    "                        for line in lines[fifth:-fifth]:\n",
    "                            if line.startswith('Received timeout'):\n",
    "                                if current_line_is_timeout:\n",
    "                                    wrong_estimates += 1\n",
    "                                else:\n",
    "                                    current_line_is_timeout = True\n",
    "                                timeouts += 1\n",
    "                                continue\n",
    "                            else:\n",
    "                                current_line_is_timeout = False\n",
    "                            m=re.search(',(.+?)[(reads/s)|(read pairs/s)]',line)\n",
    "                            if m == None:\n",
    "                                continue #EOF\n",
    "                            try:\n",
    "                                if '?' in m.group(1):\n",
    "                                    continue\n",
    "                                rates.append(float(m.group(1)))\n",
    "                            except:\n",
    "                                #No rate estimate yet\n",
    "                                if 'reads/s' in line:\n",
    "                                    rates.append(0)\n",
    "                                else:\n",
    "                                    print(s,c,cidx,line,m.group(1))\n",
    "                                    assert(False)\n",
    "\n",
    "                mean_client_rate = np.mean(rates)\n",
    "                std_client_rate = np.std(rates)\n",
    "\n",
    "                vm_data = pd.read_csv(f'benchmarks_client_counts/benchmark_{s}_{c}_vm.txt', \n",
    "                                      delim_whitespace=True, header=None,skiprows=2, names=[\n",
    "         'r'  ,'b'  , 'swpd' ,'free' ,  'buff', 'cache','si',\n",
    "                                          'so','bi','bo' ,'in', 'cs' ,'us', 'sy' ,'id', 'wa', 'st','date','time'           \n",
    "                ])\n",
    "\n",
    "                mean_user_cpu = np.mean(vm_data['us'])\n",
    "                std_user_cpu = np.std(vm_data['us'])\n",
    "\n",
    "                mean_kernel_cpu = np.mean(vm_data['sy'])\n",
    "                std_kernel_cpu = np.std(vm_data['sy'])\n",
    "\n",
    "                mean_idle_cpu = np.mean(vm_data['id'])\n",
    "                std_idle_cpu = np.std(vm_data['id'])\n",
    "\n",
    "                mean_waitio_cpu = np.mean(vm_data['wa'])\n",
    "                std_waitio_cpu = np.std(vm_data['wa'])\n",
    "\n",
    "                serv_rates = []\n",
    "                lines = if_file.readlines()\n",
    "                fifth = len(lines)//5       \n",
    "                for line in lines[fifth:-fifth]:\n",
    "                    data = line.split()[5]\n",
    "                    if data.endswith('Kb'):\n",
    "                        value /= 1024\n",
    "                        value = float(data[:-2])\n",
    "                    elif data.endswith('Mb'):\n",
    "                        value = float(data[:-2])\n",
    "                    elif data.endswith('b'):\n",
    "                        value = float(data[:-1])\n",
    "                        value /= (1024*1024)\n",
    "                    else:\n",
    "                        #Something went wrong\n",
    "                        print(data)\n",
    "                        assert(False)\n",
    "                    serv_rates.append(value)\n",
    "                mean_serv_rate = np.mean(serv_rates)\n",
    "                std_serv_rate = np.std(serv_rates)\n",
    "\n",
    "                server_rate_bases = -1\n",
    "                with open(f'benchmarks_client_counts/benchmark_{s}_{c}_bs.txt','r') as bsfile:\n",
    "                    server_rate_bases = int(bsfile.read()[1:-2])\n",
    "\n",
    "                tdf = pd.read_csv(f'benchmarks_client_counts/benchmark_{s}_{c}_do.txt',\n",
    "                                  sep='\\t',skiprows=[-1],\n",
    "                                 header=None,names=['Container','CPU','NetIO','BlockIO']).dropna()\n",
    "                tdf['Service'] = tdf['Container'].apply(lambda x: x.split('-')[-2])\n",
    "                tdf['CPU'] = tdf['CPU'].apply(lambda x: float(x[:-1]))\n",
    "\n",
    "                cpumeans = tdf.groupby('Service')[['CPU']].mean()\n",
    "                tuples.append(\n",
    "                    (\n",
    "                        c,\n",
    "                        s,\n",
    "                        mean_client_rate,\n",
    "                        std_client_rate,\n",
    "                        mean_user_cpu,\n",
    "                        std_user_cpu,\n",
    "                        mean_kernel_cpu,\n",
    "                        std_kernel_cpu,\n",
    "                        mean_idle_cpu,\n",
    "                        std_idle_cpu,\n",
    "                        mean_waitio_cpu,\n",
    "                        std_waitio_cpu,\n",
    "                        mean_serv_rate,\n",
    "                        std_serv_rate,\n",
    "                        server_rate_bases,\n",
    "                        timeouts,\n",
    "                        dead_clients,\n",
    "                        wrong_estimates,\n",
    "                        cpumeans.loc['redis']['CPU'],\n",
    "                        cpumeans.loc['api']['CPU'],\n",
    "                        cpumeans.loc['traefik']['CPU'],\n",
    "                        cpumeans.loc['filter']['CPU']\n",
    "                    )\n",
    "                )\n",
    "        except:\n",
    "            print(c,s)\n",
    "df = pd.DataFrame(tuples,columns=[\n",
    "    'Clients',\n",
    "    'Scenario',\n",
    "    'Client Reads/s Rate (Mean)',\n",
    "    'Client Reads/s Rate (Std. Dev.)',\n",
    "    'Server CPU Usage User (Mean)',\n",
    "    'Server CPU Usage User (Std. Dev.)',\n",
    "    'Server CPU Usage Kernel (Mean)',\n",
    "    'Server CPU Usage Kernel (Std. Dev.)',\n",
    "    'Server CPU Usage Idle (Mean)',\n",
    "    'Server CPU Usage Idle (Std. Dev.)',\n",
    "    'Server CPU Usage Wait I/O (Mean)',\n",
    "    'Server CPU Usage wait I/O (Std. Dev.)',\n",
    "    'Server Bandwith Usage MByte/s (Mean)',\n",
    "    'Server Bandwith Usage MByte/s (Std. Dev.)',\n",
    "    'Server Total Bases Processed',\n",
    "    'Timeouts',\n",
    "    'Dead Clients',\n",
    "    'Wrong Estimates',\n",
    "    'CPU Redis',\n",
    "    'CPU API',\n",
    "    'CPU Traefik',\n",
    "    'CPU Filter'\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f2dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Scenario Description'] = df['Scenario'].map(description_map)\n",
    "df['Chunk'] = df['Scenario Description'].apply(lambda x: x.split()[0])\n",
    "df['Throughput MBases/s'] = df['Server Total Bases Processed']/(MEASUREMENT_PERIOD*1_000_000)\n",
    "df['MBases/s processed per Client'] = df['Server Total Bases Processed']/(df['Clients']*MEASUREMENT_PERIOD*1_000_000)\n",
    "df['Server CPU Usage User (Mean) + Kernel (Mean)'] = df['Server CPU Usage User (Mean)']+df['Server CPU Usage Kernel (Mean)']\n",
    "df['Threads'] = df['Scenario Description'].apply(lambda x : int(x.split()[-2]))\n",
    "df['Wrong Estimates Fraction'] = df['Wrong Estimates']/df['Timeouts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a74b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_chart(frame):\n",
    "    charts = []\n",
    "    for measurement in [\n",
    "        'Throughput MBases/s',\n",
    "        'MBases/s processed per Client',\n",
    "        'Server CPU Usage User (Mean) + Kernel (Mean)',\n",
    "        'Server Bandwith Usage MByte/s (Mean)'\n",
    "    ]:\n",
    "        c = alt.Chart(frame).mark_line().encode(\n",
    "            x='Clients',\n",
    "            y=str(measurement),\n",
    "            color=alt.Color('Scenario Description:N',sort=[\n",
    "                '2.500 Chunk Size 10.000 Buffer Size MRSA Illumina 7 Threads',\n",
    "                '2.500 Chunk Size 10.000 Buffer Size MRSA Illumina 3 Threads',\n",
    "                '25.000 Chunk Size 100.000 Buffer Size COV ONT 7 Threads',\n",
    "                '25.000 Chunk Size 100.000 Buffer Size COV ONT 3 Threads'\n",
    "            ])\n",
    "        )\n",
    "        charts.append(c)\n",
    "    return reduce(lambda x,y : x|y,charts)\n",
    "\n",
    "c1 = gen_chart(df[df['Chunk'] == '2.500'])\n",
    "c2 = gen_chart(df[df['Chunk'] != '2.500'])\n",
    "\n",
    "\n",
    "(c1&c2).configure_legend(labelLimit=0,orient='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2104d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subframe = df\n",
    "\n",
    "\n",
    "charts = []\n",
    "for measurement in [\n",
    "    'Dead Clients',\n",
    "    'Timeouts',\n",
    "    'MBases/s processed per Client',\n",
    "    'Server CPU Utilization (%)',\n",
    "    'Server Bandwith Usage MByte/s (Mean)',\n",
    "    'Wrong Estimates Fraction',\n",
    "    'CPU Traefik',\n",
    "    'CPU Redis',\n",
    "    'CPU API',\n",
    "    'CPU Filter'\n",
    "]:\n",
    "    c = alt.Chart(subframe).mark_rect().encode(\n",
    "        x='Threads:O',\n",
    "        y='Clients:O',\n",
    "        color=str(measurement),\n",
    "        tooltip=[str(measurement)]\n",
    "    )\n",
    "    charts.append(c)\n",
    "    \n",
    "reduce(lambda x,y : x|y,charts).configure_legend(labelLimit=0,orient='bottom').resolve_scale(color='independent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f9da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('BenchmarkSpeedClients.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2ed759",
   "metadata": {},
   "outputs": [],
   "source": [
    "charts = []\n",
    "for measurement in [\n",
    "     'Server CPU Usage User (Mean)',\n",
    "     'Server CPU Usage Kernel (Mean)',\n",
    "     'Server CPU Usage Idle (Mean)',\n",
    "     'Senrver CPU Usage Wait I/O (Mean)'\n",
    "]:\n",
    "    c = alt.Chart(df).mark_line().encode(\n",
    "        x='Clients',\n",
    "        y=str(measurement),\n",
    "        color='Scenario Description:N'\n",
    "    )\n",
    "    charts.append(c)\n",
    "    \n",
    "reduce(lambda x,y : x|y,charts).configure_legend(labelLimit=0,orient='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0eeb6e",
   "metadata": {},
   "source": [
    "## Comparability of Operation Modes w.r.t. Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1972ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tuples = []\n",
    "\n",
    "#We need to evaluate all three operation modes\n",
    "for method in ['NEGATIVE','POSITIVE','COMBINED']:\n",
    "    for pathogen in ['cov']:\n",
    "        for technology in ['ont']:\n",
    "            buffer_size = 3_000_000\n",
    "            chunk_size = 100_000\n",
    "            #For Illumina WGS we only need one experiment as this is not dependent on buffer size\n",
    "            print(method,pathogen,technology)\n",
    "            client = paramiko.SSHClient()\n",
    "            client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "            try:\n",
    "\n",
    "                client.connect(hostname=server_machine, username='root')\n",
    "                FILTER_MODE = 'NEGATIVE' if method == 'NEGATIVE' else 'COMBINED'\n",
    "                \n",
    "                cmd='sed -i -E \"s|(FILTER_MODE:\\\\sstr\\\\s=\\\\s)(.+)|\\\\1\\''+\\\n",
    "                FILTER_MODE+'\\'|\" ../swgts/input/config_filter.py'\n",
    "                client.exec_command(cmd)\n",
    "                cmd= 'sed -i -E \"s|(MINIMAP2_POSITIVE_CONTIG:\\\\sstr\\\\s=\\\\s)(.+)|\\\\1\\''+\\\n",
    "                TARGET_CONTIGS[pathogen]+'\\'|\" ../swgts/input/config_filter.py'\n",
    "                client.exec_command(cmd)\n",
    "                cmd='sed -i -E \"s|(MAPPING_PRESET:\\\\sstr\\\\s=\\\\s)(.+)|\\\\1\\''+MAPPING_PRESETS[technology]+'\\'|\" ../swgts/input/config_filter.py'\n",
    "                client.exec_command(cmd)\n",
    "                cmd='sed -i -E \"s|(\\'databases/)(.+)(\\.mmi\\'\\))|\\\\1'+\\\n",
    "                    DATABASES[method][pathogen]+\\\n",
    "                    '\\\\3|\" ../swgts/input/config_filter.py'\n",
    "                client.exec_command(cmd)\n",
    "\n",
    "                cmd ='sed -i -E \"s|(MAXIMUM_PENDING_BYTES:\\\\sint\\\\s=\\\\s)(.+)|\\\\1'+str(buffer_size)+'|\" /srv/swgts-deploy/input/config_api.py'\n",
    "\n",
    "                client.exec_command(cmd)\n",
    "\n",
    "                stdin, stdout, stderr = client.exec_command(\n",
    "                    'docker compose --project-directory /srv/swgts-deploy down -v'\n",
    "                )\n",
    "                stdout.readlines() #Pseudo-Block\n",
    "                stdin, stdout, stderr = client.exec_command(\n",
    "                    'docker compose --project-directory /srv/swgts-deploy up --detach --wait'\n",
    "                )                \n",
    "                stdout.readlines() #Pseudo-Block\n",
    "                sleep(12)\n",
    "                client.close()\n",
    "            except:\n",
    "                pass\n",
    "            for repeat in range(1,4):\n",
    "            \n",
    "                #Run Client \n",
    "                pathprefix = '../../rebenching_revision/'\n",
    "\n",
    "                cmd = f'. ~/miniforge3/etc/profile.d/conda.sh && conda activate swgts-submit && cd ~/sgftp_sandbox/swgts/swgts-submit/ && python3 -m swgts-submit --count {chunk_size} --server https://{server_machine}/api {pathprefix}{pathogen}_1_{technology}.fq {add_args_wgs} --verbose'\n",
    "                now = time.time()\n",
    "                completed_process = subprocess.run(cmd,shell=True)\n",
    "                elapsed = time.time()-now\n",
    "                time_tuples.append((elapsed,method,pathogen,technology,repeat,chunk_size,buffer_size))\n",
    "                if completed_process.returncode != 0:\n",
    "                    print(completed_process)\n",
    "                    assert(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba02938",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_tuples = []\n",
    "\n",
    "#Parse exact base counts for rates\n",
    "for pathogen in ['cov']:\n",
    "    #ont\n",
    "    total = 0\n",
    "    server = {x:0 for x in BUFFER_SIZES}\n",
    "    for read in SeqIO.parse(f'{pathogen}_1_ont.fq','fastq'):\n",
    "        lengths_histo.append((pathogen,len(read),read.id.endswith('hom')))\n",
    "        total += len(read)\n",
    "        for buffer_size in BUFFER_SIZES:\n",
    "            if len(read) <= buffer_size:\n",
    "                server[buffer_size] += len(read)\n",
    "    length_tuples.append((pathogen,'ont',total))\n",
    "length_df = pd.DataFrame(length_tuples,columns=['Pathogen','Technology','Bases'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04b9760",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = pd.DataFrame(\n",
    "    time_tuples,\n",
    "    columns=[\n",
    "        'Time','Mode','Pathogen','Technology','Repeat','Chunk','Buffer'\n",
    "    ]\n",
    ")\n",
    "time_df = pd.merge(time_df,length_df,on=['Pathogen','Technology'],how='left')\n",
    "time_df['Bases/s'] = time_df['Bases']/time_df['Time']\n",
    "time_df.groupby('Mode')['Bases/s'].mean()\n",
    "time_df['Mode'] = time_df['Mode'].map({\n",
    "    'NEGATIVE' : 'Host Subtraction',\n",
    "    'POSITIVE' : 'Simple Pathogen Retention',\n",
    "    'COMBINED' : 'Host-Competitive Pathogen Retention'\n",
    "})\n",
    "time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd19ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(time_df).mark_point().encode(\n",
    "    x='Mode',\n",
    "    y='Bases/s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d149c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
